@@@@@@@@@@@@@ Backpropagation
vanishing gradient : 
깊이가 깊어질수록 미치는 영향력이 미비해짐
즉 출력층레이어값에서 입력층 레이어로 갈수록 영향을 미치지못함

ReLU : Rectified Linear Unit 
아래와 같이 0이하로는 0 이상으로는 쭈욱 증가하게끔해주는 함수 max(0,x)


       /
      /
     /
0----
    0

## 깊어질수록 ReLU 함수를 사용하고 최종적으로 sigmoid 사용함

그외에도 tanh : sigmoid 를 -1 ~ 1까지 값으로 표현
        Leaky ReLU : max(0.1x,x) 0이하의 값을 0.1비율로 사용
        Maxout , ELU 등...
