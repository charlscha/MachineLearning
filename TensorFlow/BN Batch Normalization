# 배치 정규화 Batch Normalization

학습의 효율을 높이기위해
1. 학습속도 개선 (학습률을 높게 설정할수있기때문)
2. 가중치 초기값 선택의 의존성이 적어짐(학습할때 마다 출력값을 정규화)
3. 과적합(overfitting)위험을 줄임
4. Gradient Vanishing 문제해결


>> 깊이가 깊은 층의 경우 입력의 안정도가 떨어진다 즉, 앞 레이어의 가중치에 영향을 받으므로
  결론은 이전 층의 출력을 표준화 한다는 의미가된다.
